{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0afce6b2-19b2-484c-8064-e9d5c6ed6881",
   "metadata": {},
   "source": [
    "# Global Healthy and Sustainable City Indicators (GHSCI)\n",
    "\n",
    "## Cycling Indicators\n",
    "\n",
    "This notebook prepares the inputs required to calculate cycling indicators. It builds a multimodal transport network (walking and cycling), generates sample points that represent origins, and compiles destination datasets covering public transport stops, public open space, and fresh-food markets. Details of the OSM tags and destination categories are provided in the [OSM destination definitions](https://github.com/healthysustainablecities/global-indicators/blob/main/process/configuration/templates/osm_destination_definitions.csv) folder .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f098f2-b8d4-48b3-8936-be8ca8a327c0",
   "metadata": {},
   "source": [
    "Step 1: Create the configuration file\n",
    "\n",
    "Use the GHSCI software to create the intended city’s configuration file from the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8ef41-a1c9-4396-80b4-62ccb31fde0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ghsci \n",
    "codename = \"Maribyrnong\" \n",
    "ghsci.configure(codename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96fe21f-bf78-42e9-8a6e-d3feb994ddc6",
   "metadata": {},
   "source": [
    "Step 2: Adjust the configuration file\n",
    "\n",
    "Update the configuration so it is appropriate for the intended city. For example, set the correct paths to the OSM data and population datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6765a45b-a0fe-4558-aa13-8e317ed55f00",
   "metadata": {},
   "source": [
    "Step 3: Run the analysis\n",
    "\n",
    "Load the intended city’s region, then run the analysis function to generate the required datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c3182-938e-4f52-b08d-8f13a30f612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ghsci.Region(codename) \n",
    "r.analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3699abb-7a40-470c-9cb6-d3542b06bd01",
   "metadata": {},
   "source": [
    "Step 4: Save the datasets\n",
    "\n",
    "Export the generated datasets for use in the cycling indicator generation workflow. The code retrieves the active travel network, origins and destinations from Postgres and writes them to GeoPackages:\n",
    "1. network.gpkg\n",
    "2. origins.gpkg\n",
    "3. destinations.gpkg\n",
    "\n",
    "It then copies these files to the local environment, making them available for downstream processes and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0887b4-93c0-4982-8c8d-c5486f6aa932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# ========= Config =========\n",
    "PG_URL = \"postgresql+psycopg2://postgres:ghscic@gateway.docker.internal:5433/maribyrnong\"\n",
    "ENGINE = create_engine(PG_URL)\n",
    "\n",
    "OUT_DIR = f\"/data/_study_region_outputs/{codename}\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "NETWORK_GPKG = os.path.join(OUT_DIR, f\"{codename}_network.gpkg\")\n",
    "ORIG_GPKG    = os.path.join(OUT_DIR, f\"{codename}_origins.gpkg\")\n",
    "DEST_GPKG    = os.path.join(OUT_DIR, f\"{codename}_destinations.gpkg\")\n",
    "\n",
    "for p in (NETWORK_GPKG, ORIG_GPKG, DEST_GPKG):\n",
    "    if os.path.exists(p):\n",
    "        os.remove(p)\n",
    "\n",
    "# ========= Helpers =========\n",
    "def list_spatial_tables(schema=\"public\"):\n",
    "    sql = text(\"\"\"\n",
    "        SELECT f_table_schema AS schema, f_table_name AS name, f_geometry_column AS geom_col\n",
    "        FROM public.geometry_columns\n",
    "        WHERE f_table_schema = :s\n",
    "        GROUP BY f_table_schema, f_table_name, f_geometry_column\n",
    "        ORDER BY f_table_name\n",
    "    \"\"\")\n",
    "    with ENGINE.connect() as con:\n",
    "        return pd.read_sql_query(sql, con, params={\"s\": schema})\n",
    "\n",
    "def has_column(schema, table, col):\n",
    "    sql = text(\"\"\"\n",
    "        SELECT 1\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema=:s AND table_name=:t AND column_name=:c\n",
    "        LIMIT 1\n",
    "    \"\"\")\n",
    "    with ENGINE.connect() as con:\n",
    "        return pd.read_sql_query(sql, con, params={\"s\": schema, \"t\": table, \"c\": col}).shape[0] > 0\n",
    "\n",
    "def non_geom_cols(schema, table, geom_cols=(\"geom\",\"geom_4326\")):\n",
    "    sql = text(\"\"\"\n",
    "        SELECT column_name\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema=:s AND table_name=:t\n",
    "          AND column_name <> ALL(:gcols)\n",
    "        ORDER BY ordinal_position\n",
    "    \"\"\")\n",
    "    with ENGINE.connect() as con:\n",
    "        cols = pd.read_sql_query(sql, con, params={\"s\": schema, \"t\": table, \"gcols\": list(geom_cols)})\n",
    "    return [f'\"{c}\"' for c in cols[\"column_name\"].tolist()]\n",
    "\n",
    "def read_table(schema, table):\n",
    "    \"\"\"Load any spatial table; prefer geom_4326 else transform geom to 4326.\"\"\"\n",
    "    cols_non_geom = \", \".join(non_geom_cols(schema, table))\n",
    "    select_cols = (cols_non_geom + \", \") if cols_non_geom else \"\"\n",
    "    if has_column(schema, table, \"geom_4326\"):\n",
    "        sql = text(f'SELECT {select_cols}\"geom_4326\" FROM \"{schema}\".\"{table}\"')\n",
    "        geom_col = \"geom_4326\"\n",
    "    else:\n",
    "        sql = text(f'SELECT {select_cols}ST_Transform(\"geom\", 4326) AS geom_4326 FROM \"{schema}\".\"{table}\"')\n",
    "        geom_col = \"geom_4326\"\n",
    "    with ENGINE.connect() as con:\n",
    "        gdf = gpd.read_postgis(sql, con, geom_col=geom_col)\n",
    "\n",
    "    # Make wierd dtypes strings so OGR keeps them\n",
    "    for c in gdf.columns:\n",
    "        if c != geom_col and not (\n",
    "            pd.api.types.is_integer_dtype(gdf[c]) or\n",
    "            pd.api.types.is_float_dtype(gdf[c]) or\n",
    "            pd.api.types.is_bool_dtype(gdf[c]) or\n",
    "            pd.api.types.is_string_dtype(gdf[c])\n",
    "        ):\n",
    "            gdf[c] = gdf[c].map(lambda x: None if x is None else str(x))\n",
    "\n",
    "    return gdf.set_crs(4326, allow_override=True)\n",
    "\n",
    "def save_layer(gdf, gpkg_path, layer_name):\n",
    "    if gdf.empty:\n",
    "        print(f\" - skip {layer_name}: empty\")\n",
    "        return\n",
    "    mode = \"w\" if not os.path.exists(gpkg_path) else \"a\"\n",
    "    # Light name tidy so QGIS is happy\n",
    "    gdf = gdf.rename(columns={c: c.replace(\":\", \"_\") for c in gdf.columns})\n",
    "    gdf.to_file(gpkg_path, layer=layer_name, driver=\"GPKG\", mode=mode)\n",
    "    print(f\" + wrote {layer_name} -> {gpkg_path} ({len(gdf)} features)\")\n",
    "\n",
    "def match_any(name, pats):\n",
    "    return any(re.search(p, name, flags=re.IGNORECASE) for p in pats)\n",
    "\n",
    "# ========= Discover patterns =========\n",
    "ORIGIN_PATTERNS = [r\"origin\", r\"origins\", r\"sample_point\", r\"population_point\", r\"grid_origin\", r\"pop_grid\"]\n",
    "DEST_PATTERNS = [\n",
    "    r\"dest\", r\"destination\",\n",
    "    r\"fresh[_\\- ]?food\", r\"grocery\", r\"supermarket\", r\"market\",\n",
    "    r\"public[_\\- ]?open[_\\- ]?space\", r\"\\bpos\\b\", r\"open[_\\- ]?space\", r\"park\",\n",
    "    r\"gtfs\", r\"stop\", r\"stops\", r\"station\", r\"transit\", r\"pt[_\\- ]?stop\", r\"tram\", r\"bus\", r\"rail\",\n",
    "]\n",
    "NETWORK_PATTERNS = [\n",
    "    (\"public\", \"edges\", \"edges\"),\n",
    "    (\"public\", \"edges_simplified\", \"edges_simplified\"),\n",
    "    (\"public\", \"nodes\", \"nodes\"),\n",
    "    (\"public\", \"intersections_osmnx_12m\", \"intersections\"),\n",
    "]\n",
    "\n",
    "# ========= Exporters =========\n",
    "def export_network(gpkg_path=NETWORK_GPKG, schema=\"public\"): \n",
    "\n",
    "    for s, t, layer in NETWORK_PATTERNS:\n",
    "        gdf = read_table(s, t)\n",
    "        save_layer(gdf, gpkg_path, layer)\n",
    "\n",
    "def export_origins_and_destinations(schema=\"public\"):\n",
    "    candidates = list_spatial_tables(schema)\n",
    "    \n",
    "    origin_tables = sorted([t for t in candidates[\"name\"] if match_any(t, ORIGIN_PATTERNS)])\n",
    "    dest_tables   = sorted([t for t in candidates[\"name\"] if match_any(t, DEST_PATTERNS)])\n",
    "\n",
    "    print(\"Origin candidates:\", origin_tables)\n",
    "    print(\"Destination candidates:\", dest_tables)\n",
    "\n",
    "    for t in origin_tables:\n",
    "        try:\n",
    "            gdf = read_table(schema, t)\n",
    "            if not gdf.empty:\n",
    "                save_layer(gdf, ORIG_GPKG, t)\n",
    "        except Exception as e:\n",
    "            print(f\" ! origin '{t}' failed: {e}\")\n",
    "\n",
    "    for t in dest_tables:\n",
    "        try:\n",
    "            gdf = read_table(schema, t)\n",
    "            if not gdf.empty:\n",
    "                save_layer(gdf, DEST_GPKG, t)\n",
    "        except Exception as e:\n",
    "            print(f\" ! destination '{t}' failed: {e}\")\n",
    "\n",
    "# ========= Run =========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Exporting network…\")\n",
    "    export_network()\n",
    "    print(\"\\nExporting origins/destinations…\")\n",
    "    export_origins_and_destinations()\n",
    "    print(\"\\nDone.\")\n",
    "    print(f\"Network GPKG:      {NETWORK_GPKG}   (exists: {os.path.exists(NETWORK_GPKG)})\")\n",
    "    print(f\"Origins GPKG:      {ORIG_GPKG}      (exists: {os.path.exists(ORIG_GPKG)})\")\n",
    "    print(f\"Destinations GPKG: {DEST_GPKG}      (exists: {os.path.exists(DEST_GPKG)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0072366-21a1-4a7f-bc7c-763d402a2390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from typing import Optional, Dict\n",
    "\n",
    "# ========= Helper =========\n",
    "def copy_outputs_to_container(\n",
    "    codename: str,\n",
    "    network_gpkg: str,\n",
    "    origins_gpkg: str,\n",
    "    destinations_gpkg: str,\n",
    "    base_dst: str = \"/home/ghsci/process/data/_study_region_outputs\",\n",
    ") -> Dict[str, Optional[str]]:\n",
    "    \n",
    "    dst_dir = os.path.join(base_dst, codename)\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "    results = {}\n",
    "    for src in (network_gpkg, origins_gpkg, destinations_gpkg):\n",
    "        label = os.path.basename(src) if src else \"(missing path)\"\n",
    "        if not src or not os.path.exists(src):\n",
    "            print(f\"✗ Not found: {src}\")\n",
    "            results[src] = None\n",
    "            continue\n",
    "        dst = os.path.join(dst_dir, os.path.basename(src))\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f\"Copied {label} -> {dst}\")\n",
    "        results[src] = dst\n",
    "\n",
    "    print(f\"\\nDone. Target folder: {dst_dir}\")\n",
    "    return results\n",
    "\n",
    "# ========= Run =========\n",
    "copy_outputs_to_container(codename, NETWORK_GPKG, ORIG_GPKG, DEST_GPKG)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
