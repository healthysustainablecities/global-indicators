{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create frequent public transport stops analysis - Phoenix, Arizona\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes a process to retain public transit stops points with frequent service using two methods.\n",
    "\n",
    "## Method 2: Stop headway analysis using average frequencies of departure\n",
    "\n",
    "- group by service date, then count the frequencies of departure for each stop\n",
    "- calculate the average headway during the timeframe over departure\n",
    "- approach taken by Carl Higgs (20200106) following [tidytransit](http://tidytransit.r-transit.org/articles/servicepatterns.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "- Input data format, including which tables, which columns, which order and handling of nulls - you'll need to customise this for each city.  A handy guide to the format of GTFS fields is [here](https://developers.google.com/transit/gtfs/reference/#tripstxt).\n",
    "\n",
    "- GTFS feeds sourced in same season (?) and school term\n",
    "    - we have vastly different climates across our cities --- so should perhaps aim for Spring? if we chose arbitrary feeds, some in winter and some in summer, our different cities could have vastly different conditions (particularly the snowy ones)\n",
    "- Week days (Monday to Friday)\n",
    "    - Day time (7am to 7pm (peak hour: 7am to 9am); as travel during day is important for different segments of community)\n",
    " \n",
    "- We may need to consider details of how headway is operationalised (ie. spot checks do the results look reasonable, and if not how do we deal with it?  this is why I took the maximum headway of the two directions as it at the time seemed a simple and reasonable approach to recognise that a stop need only be frequently serviced in one direction - which is not literally interpreted as 'direction on the road', its do with the route itself.  There may be other methods to consider.)\n",
    "\n",
    "Previous working branch:\n",
    "\n",
    "https://github.com/shiqin-liu/global-indicators/blob/phoenix/process/07_create_frequent_public_transport.ipynb  \n",
    "https://github.com/shiqin-liu/global-indicators/blob/f86f4c2ca068a61f9f2621351f1ce9bbf4b04d88/process/07_create_public_transport_points.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c udst urbanaccess\n",
    "#For UrbanAccess installation instructions see: https://udst.github.io/urbanaccess/installation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install geopandas=0.6.3 -c conda-forge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandana as pdna\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import time   \n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import calendar\n",
    "import datetime\n",
    "\n",
    "import urbanaccess as ua\n",
    "from urbanaccess.config import settings\n",
    "from urbanaccess.gtfsfeeds import feeds\n",
    "from urbanaccess import gtfsfeeds\n",
    "from urbanaccess.gtfs.gtfsfeeds_dataframe import gtfsfeeds_dfs\n",
    "\n",
    "import ua_load\n",
    "\n",
    "\n",
    "from shapely.geometry import shape,Point, LineString, Polygon\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpd.__version__ # UrbanAccess required earlier version of geopandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandana currently uses depreciated parameters in matplotlib, this hides the warning until its fixed\n",
    "import warnings\n",
    "import matplotlib.cbook\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffered Adelaide  :  (138.46098212857206, -35.15966609024628, 138.74830806651352, -34.71454282915053)\n",
      "Buffered Melbourne  :  (144.59067957842007, -38.21131973169178, 145.39847326519424, -37.61837232908795)\n",
      "Buffered Sydney  :  (150.6290606117829, -34.12321411958463, 151.3206735172292, -33.66275213092711)\n"
     ]
    }
   ],
   "source": [
    "#get lat lon bounding box for each study region\n",
    "for layer in ['Buffered Adelaide', 'Buffered Melbourne', 'Buffered Sydney']:\n",
    "    crs = 'epsg:7845'\n",
    "    # load shapefile\n",
    "    shape = gpd.GeoDataFrame.from_file('data/Transport/Australian stops for reference/gtfs_2020_02_14_headway_au_global_indicators.gpkg', \n",
    "                  layer=layer)\n",
    "    # create buffer\n",
    "    polygon = shape['geometry'].iloc[0]\n",
    "    polygon_proj = ox.project_geometry(polygon, crs=crs, to_crs=None, to_latlong=True)\n",
    "    # get bounding box\n",
    "    bbox = polygon_proj[0].bounds\n",
    "    print(layer, ' : ', bbox)\n",
    "    \n",
    "#minx , miny , maxx , maxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up study region GTFS config\n",
    "GTFS = {\n",
    "        'adelaide':{'gtfs_filename':'data/Transport/2019/gtfs_au_sa_adelaidemetro_20191004',\n",
    "                    'gtfs_provider' : 'Adelaide Metro',\n",
    "                    'gtfs_year' : '2019',\n",
    "                    # define month and day for \"representative period\" ie. not in school time\n",
    "                    'start_date_mmdd' : '20191008', \n",
    "                    'end_date_mmdd' : '20191205',\n",
    "                    # get bounding box from study region boundary shapefile\n",
    "                    # bounding box formatted as a 4 element tuple: (lng_max, lat_min, lng_min, lat_max)\n",
    "                    # you can generate a bounding box by going to http://boundingbox.klokantech.com/ and selecting the CSV format.\n",
    "                    'bbox' : (138.46098212857206, -35.15966609024628, 138.74830806651352, -34.71454282915053), \n",
    "                    # define modes for GTFS feed(s) as per agency_id codes in agency.txt below\n",
    "                    'modes' : {\n",
    "                        'tram':{'route_types': [0], \n",
    "                                'peak_hour' : ['07:00:00', '09:00:00'],\n",
    "                                'day_hour' : ['07:00:00', '19:00:00'],\n",
    "                                'intervals': 30,\n",
    "                                'agency_id': None},\n",
    "                        'bus' : {'route_types': [3],\n",
    "                                  'peak_hour' : ['07:00:00', '09:00:00'], \n",
    "                                  'day_hour' : ['07:00:00', '19:00:00'],\n",
    "                                  'intervals': 30,\n",
    "                                 'agency_id': None}\n",
    "                    }\n",
    "                   },\n",
    "        'melbourne':{'gtfs_filename':'data/Transport/2019/gtfs_au_vic_ptv_20191004',\n",
    "                    'gtfs_provider' : 'Public Transport Victoria',\n",
    "                    'gtfs_year' : '2019',\n",
    "                    # define month and day for \"representative period\" ie. not in school time\n",
    "                    'start_date_mmdd' : '20191008', \n",
    "                    'end_date_mmdd' : '20191205',\n",
    "                     'bbox' : (144.59067957842007, -38.21131973169178, 145.39847326519424, -37.61837232908795),\n",
    "                    # define modes for GTFS feed(s) as per agency_id codes in agency.txt below\n",
    "                    'modes' : {\n",
    "                        'tram':{'route_types': [0], \n",
    "                                'peak_hour' : ['07:00:00', '09:00:00'],\n",
    "                                'day_hour' : ['07:00:00', '19:00:00'],\n",
    "                                'intervals': 30,\n",
    "                                'agency_id': [3]},\n",
    "                        'bus' : {'route_types': [3],\n",
    "                                  'peak_hour' : ['07:00:00', '09:00:00'], \n",
    "                                  'day_hour' : ['07:00:00', '19:00:00'],\n",
    "                                  'intervals': 30,\n",
    "                                 'agency_id': [4, 6]}\n",
    "                    }\n",
    "                   },\n",
    "        'sydney' : {'gtfs_filename':'data/Transport/2019/gtfs_au_nsw_tfnsw_complete_20190619',\n",
    "                    'gtfs_provider' : 'Transport for NSW',\n",
    "                    'gtfs_year' : '2019',\n",
    "                    # define month and day for \"representative period\" ie. not in school time\n",
    "                    'start_date_mmdd' : '20191008', \n",
    "                    'end_date_mmdd' : '20191205',\n",
    "                    'bbox' : (150.6290606117829, -34.12321411958463, 151.3206735172292, -33.66275213092711),\n",
    "                    # define modes for GTFS feed(s) as per agency_id codes in agency.txt below\n",
    "                    'modes' : {\n",
    "                        'tram':{'route_types': [0], \n",
    "                                'peak_hour' : ['07:00:00', '09:00:00'],\n",
    "                                'day_hour' : ['07:00:00', '19:00:00'],\n",
    "                                'intervals': 30,\n",
    "                                'agency_id': None},\n",
    "                        'bus' : {'route_types': [700,712,714],\n",
    "                                  'peak_hour' : ['07:00:00', '09:00:00'], \n",
    "                                  'day_hour' : ['07:00:00', '19:00:00'],\n",
    "                                  'intervals': 30,\n",
    "                                'agency_id': None}\n",
    "                    }\n",
    "                   }\n",
    "       }\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a study region config to run in this notebook\n",
    "gtfsfeed_path = GTFS['adelaide']['gtfs_filename']\n",
    "start_date=GTFS['adelaide']['start_date_mmdd']\n",
    "end_date=GTFS['adelaide']['end_date_mmdd']\n",
    "\n",
    "peak_hour = GTFS['adelaide']['modes']['bus']['peak_hour']\n",
    "start_hour = peak_hour[0]\n",
    "end_hour = peak_hour[1]\n",
    "\n",
    "headway_intervals = GTFS['adelaide']['modes']['bus']['intervals']\n",
    "route_types = GTFS['adelaide']['modes']['bus']['route_types']\n",
    "agency_ids = GTFS['adelaide']['modes']['bus']['agency_id']\n",
    "\n",
    "shape_layer_name = 'Buffered Adelaide'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GTFS data into an UrbanAccess transit data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using UrbanAccess as source for:\n",
    "- The settings object\n",
    "- The feeds object and searching for GTFS feeds\n",
    "- Downloading GTFS data\n",
    "- Loading GTFS data into a UrbanAccess transit data object\n",
    "- conduct heaway analysis using maxi interdeparture time method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revise based on origional function, take away the add agency and route type features\n",
    "loaded_feeds = ua_load.gtfsfeed_to_df(gtfsfeed_path=gtfsfeed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The transit data object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a global `urbanaccess_gtfs_df` object that can be accessed with the specified variable `loaded_feeds`. This object holds all the individual GTFS feed files aggregated together with each GTFS feed file type in separate Pandas DataFrames to represent all the loaded transit feeds in a metropolitan area. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly view the transit stop locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_feeds.stops.plot(kind='scatter', x='stop_lon', y='stop_lat', s=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate stop level headways\n",
    "- Calculate headways for the AM Peak time period \n",
    "- the original UrbanAccess function calculates the stop and route level headway, it was revised to conduct only the stop level headway analysis.\n",
    "Check here for the original [function](https://github.com/UDST/urbanaccess/blob/9c0f64faf63bae6f5a73ae23409ca68b28113026/urbanaccess/gtfs/network.py#L192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hold this condition \n",
    "#select daytime bus service\n",
    "loaded_feeds.stop_times = loaded_feeds.stop_times.sort_values(['trip_id', 'stop_sequence'])\n",
    "loaded_feeds.stop_times = loaded_feeds.stop_times[(loaded_feeds.stop_times['departure_time'] >= '07:00:00') \n",
    "                                                  & (loaded_feeds.stop_times['departure_time'] <= '19:00:00')]\n",
    "\n",
    "# select stops with ealiest service before 7:30am, and lastest stop before 18:30pm. groupby stop id\n",
    "stop_first_peak_service_bus = (loaded_feeds.stop_times.groupby(loaded_feeds.stop_times.stop_id).departure_time.min() <= '07:30:00')\n",
    "stop_last_peak_service_bus = (loaded_feeds.stop_times.groupby(loaded_feeds.stop_times.stop_id).departure_time.max() >= '18:30:00')\n",
    "\n",
    "\n",
    "loaded_feeds.stop_times = loaded_feeds.stop_times.set_index('stop_id') #set stop_id as index columns\n",
    "loaded_feeds.stop_times = loaded_feeds.stop_times[(stop_first_peak_service_bus) & (stop_last_peak_service_bus)].reset_index()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup calender\n",
    "\n",
    "- select valid service id based on weekdays, and start and end date\n",
    "- this process is developed based on tidytransit [set_date_service_table](https://github.com/r-transit/tidytransit/blob/8a207aaf81482c268aa4bfc548c0697e24b5281c/R/time.R#L59) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_weekday_df(start, end):\n",
    "    \"\"\"create table to show date and weekday of all dates from start to end date\"\"\"\n",
    "    date_range = pd.date_range(start=start, end=end)\n",
    "    dates = pd.DataFrame(date_range, columns=['date'])\n",
    "    # Return the day of the week as an integer, where Monday is 0 and Sunday is 6.\n",
    "    weekdays = pd.DataFrame(date_range.weekday, columns=['weekday'])\n",
    "    date_weekday_df = dates.join(weekdays)\n",
    "    \n",
    "    # replace weekday numeric to str values\n",
    "    weekdays = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    date_weekday_df = date_weekday_df.replace([0, 1, 2, 3, 4, 5, 6], weekdays)\n",
    "    return date_weekday_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_date_service_table(loaded_feeds):\n",
    "    \"\"\"\n",
    "    Summarize service that run on each date; \n",
    "    Use it to summarise service. For example, get a count of the number of services for a date.\n",
    "    \"\"\"\n",
    "    # tabulate each date and weekday from the start to the end date in calendar  \n",
    "    dates = get_date_weekday_df(start=str(min(loaded_feeds.calendar['start_date'])), \n",
    "                            end=str(max(loaded_feeds.calendar['end_date'])))\n",
    "    \n",
    "    # gather services by weekdays\n",
    "    service_ids_weekdays = loaded_feeds.calendar[['service_id', 'start_date', 'end_date', \n",
    "                           'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday'\n",
    "                          ]].set_index(['service_id', 'start_date', 'end_date'\n",
    "                                       ]).stack().to_frame().reset_index()\n",
    "\n",
    "    service_ids_weekdays = service_ids_weekdays[(service_ids_weekdays[0] == 1)\n",
    "                                               ].rename(columns={'level_3':'weekday'}).drop(columns=[0])\n",
    "    # create table to connect every date to corresponding services (all dates from earliest to latest)\n",
    "    # set services to dates according to weekdays and start/end date\n",
    "    date_service_df = pd.merge(dates, service_ids_weekdays, on='weekday')\n",
    "\n",
    "    date_service_df['start_date'] = pd.to_datetime(date_service_df['start_date'], format='%Y%m%d')\n",
    "    date_service_df['end_date'] = pd.to_datetime(date_service_df['end_date'], format='%Y%m%d')\n",
    "\n",
    "\n",
    "    #filter valid service date within start and end date\n",
    "    date_service_df = date_service_df[(date_service_df['date'] >= date_service_df['start_date']) \n",
    "                    & (date_service_df['date'] <= date_service_df['end_date'])][['date', 'weekday', 'service_id']]\n",
    "    \n",
    "    \n",
    "    # add calendar_dates additions (1) if the additional dates are within the start and end date range\n",
    "    addition_dates = loaded_feeds.calendar_dates[(loaded_feeds.calendar_dates['exception_type']==1)\n",
    "                                                ][['service_id', 'date']]\n",
    "\n",
    "    min_start_datetime = pd.to_datetime(str(min(loaded_feeds.calendar['start_date'])), format='%Y%m%d')\n",
    "    max_end_datetime = pd.to_datetime(str(max(loaded_feeds.calendar['end_date'])), format='%Y%m%d')\n",
    "    addition_dates['date'] = pd.to_datetime(addition_dates['date'], format='%Y%m%d')\n",
    "\n",
    "    addition_dates['within_range'] = addition_dates['date'].apply(\n",
    "        lambda x: 1 if (x >= min_start_datetime\n",
    "                       ) & (x <= max_end_datetime) else 0)\n",
    "\n",
    "    addition_dates = addition_dates[addition_dates['within_range'] == 1][['service_id', 'date']]\n",
    "    date_service_df = pd.concat([addition_dates, date_service_df], ignore_index=True)\n",
    "    \n",
    "    # remove calendar_dates exceptions (2)\n",
    "    exception_dates = loaded_feeds.calendar_dates[(loaded_feeds.calendar_dates['exception_type']==2)\n",
    "                                                 ][['service_id', 'date']]\n",
    "\n",
    "    exception_dates['date'] = pd.to_datetime(exception_dates['date'], format='%Y%m%d')\n",
    "\n",
    "    date_service_exception_df = pd.merge(exception_dates.set_index(['service_id', 'date']\n",
    "                                      ), date_service_df.set_index(['service_id', 'date']), \n",
    "             left_index=True, right_index=True, indicator=True, how='outer').reset_index()\n",
    "\n",
    "    date_service_df = date_service_exception_df[(date_service_exception_df['_merge']=='right_only')\n",
    "                                               ][['service_id', 'date', 'weekday']]\n",
    "    return date_service_df\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop headway analysis using average departure time within the timeframe\n",
    "\n",
    "- Identify feeds start and end dates that that a stop is a 30-minute frequency stop  \n",
    "- group by service date, then count the frequencies of departure for each stop\n",
    "- calculate the average headway during the timeframe over departure\n",
    "- approach taken by Carl Higgs (20200106) following [tidytransit](http://tidytransit.r-transit.org/articles/servicepatterns.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revise based on tidytransit [get_stop_frequency function]\n",
    "# https://github.com/r-transit/tidytransit/blob/master/R/frequencies.R\n",
    "\n",
    "def get_hlc_stop_frequency(loaded_feeds, start_hour, end_hour, start_date, \n",
    "                           end_date, route_types, agency_ids, \n",
    "                           dow=['monday','tuesday','wednesday','thursday','friday']):\n",
    "    startTime = time.time()\n",
    "    # set service date\n",
    "    date_service_df = set_date_service_table(loaded_feeds)\n",
    "\n",
    "    # limit within specified start and end date, and within weekdays\n",
    "    start_date_mmdd = pd.to_datetime(start_date, format='%Y%m%d')\n",
    "    end_date_mmdd = pd.to_datetime(end_date, format='%Y%m%d')\n",
    "\n",
    "    date_service_df = date_service_df[date_service_df['weekday'].isin(dow)\n",
    "                                     ][(date_service_df['date'] >= start_date_mmdd) \n",
    "                                       & (date_service_df['date'] <= end_date_mmdd)].drop_duplicates()\n",
    "    print('     ', len(date_service_df), ' unique service dates are identified within', \n",
    "          dow, ' from ', start_date, ' to ', end_date)\n",
    "    \n",
    "    \n",
    "    trips_routes = pd.merge(loaded_feeds.trips, loaded_feeds.routes, on='route_id', how='left')\n",
    "    # filter route trips based on valid services\n",
    "    valid_service_ids = date_service_df.service_id.unique()\n",
    "    trips_routes = trips_routes[trips_routes['service_id'].isin(valid_service_ids)]\n",
    "    \n",
    "    # filter trips within route types and agency id\n",
    "    if (route_types != None) & (agency_ids == None):\n",
    "        trips_routes = trips_routes[(trips_routes['route_type'].isin(route_types))]\n",
    "        date_service_df = date_service_df[date_service_df['service_id'].isin(trips_routes.service_id.unique())]\n",
    "    elif (route_types != None) & (agency_ids != None):\n",
    "        trips_routes = trips_routes[(trips_routes['agency_id'].isin(agency_ids)) & (trips_routes['route_type'].isin(route_types))]\n",
    "        date_service_df = date_service_df[date_service_df['service_id'].isin(trips_routes.service_id.unique())]\n",
    "    elif (route_types == None) & (agency_ids != None):\n",
    "        trips_routes = trips_routes[(trips_routes['agency_id'].isin(agency_ids))]\n",
    "        date_service_df = date_service_df[date_service_df['service_id'].isin(trips_routes.service_id.unique())]\n",
    "        \n",
    "    # takes input start and end time range from 24 hour clock and converts\n",
    "    # it to seconds past midnight\n",
    "    # in order to select times that may be after midnight\n",
    "\n",
    "    # convert string time components to integer and then calculate seconds\n",
    "    # past midnight\n",
    "    # convert starttime 24 hour to seconds past midnight\n",
    "    start_h = int(str(start_hour[0:2]))\n",
    "    start_m = int(str(start_hour[3:5]))\n",
    "    start_s = int(str(start_hour[6:8]))\n",
    "    starttime_sec = (start_h * 60 * 60) + (start_m * 60) + start_s\n",
    "\n",
    "    # convert endtime 24 hour to seconds past midnight\n",
    "    end_h = int(str(end_hour[0:2]))\n",
    "    end_m = int(str(end_hour[3:5]))\n",
    "    end_s = int(str(end_hour[6:8]))\n",
    "    endtime_sec = (end_h * 60 * 60) + (end_m * 60) + end_s\n",
    "\n",
    "    stop_times = loaded_feeds.stop_times[loaded_feeds.stop_times.trip_id.isin(trips_routes.trip_id.unique())]\n",
    "\n",
    "    # filter stop times within the timerange\n",
    "    selected_stop_times_df = stop_times[((starttime_sec < loaded_feeds.stop_times[\n",
    "        'departure_time_sec'])  & (stop_times[\"departure_time_sec\"] < endtime_sec))]\n",
    "    \n",
    "    # filter valid service trips\n",
    "    valid_service_trips = pd.merge(date_service_df, trips_routes, on='service_id', how='left')\n",
    "\n",
    "    # filter stops within valid service and time range \n",
    "    stop_time_trips = pd.merge(valid_service_trips, selected_stop_times_df, on='trip_id', how='inner')\n",
    "    \n",
    "    # group by service date to get counts of departure of each stop of each direction\n",
    "    stop_time_trips_departure = stop_time_trips[['direction_id', 'stop_id', 'date', 'service_id'\n",
    "                                      ]].groupby(['direction_id', 'stop_id', 'date']).agg(['count'])\n",
    "\n",
    "    stop_time_trips_departure.columns = ['departure']\n",
    "    \n",
    "    # count sec. and min. within the timerange\n",
    "    t1_min = (endtime_sec - starttime_sec)/60\n",
    "    \n",
    "    # for each stop we average headway over dates \n",
    "    # We take the best (smallest) headway out of the two possible of the stop\n",
    "    # this is because many stops have frequent service in one direction \n",
    "    # and infrequent in the other (ie. inbound vs outbound differences)\n",
    "    stop_time_trips_departure['headway'] = round(t1_min / stop_time_trips_departure['departure'])\n",
    "    \n",
    "    stops_headway = stop_time_trips_departure.reset_index().groupby([\n",
    "    'stop_id', 'direction_id']).mean().groupby('stop_id').min()[['headway']]\n",
    "    \n",
    "    #print('     Time to complete average stop headway analysis with {} frequent stops is: {}'.format(len(stops_headway), time.time() - startTime))\n",
    "    \n",
    "    return stops_headway\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_headway = get_hlc_stop_frequency(loaded_feeds, start_hour, end_hour, start_date, \n",
    "                           end_date, route_types, agency_ids, \n",
    "                           dow=['monday','tuesday','wednesday','thursday','friday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select average departure headway less than or equal to maxi headway frequency intervals\n",
    "stop_30_mins_headway = stops_headway[stops_headway['headway'] <= headway_intervals]\n",
    "\n",
    "# get spatial features for freqent stops\n",
    "stop_30_mins_final = pd.merge(stop_30_mins_headway, loaded_feeds.stops, how='left', on='stop_id')\n",
    "\n",
    "# add stop id geometry\n",
    "stop_30_mins_final['geometry'] = stop_30_mins_final.apply(\n",
    "    lambda row: Point(row['stop_lon'], row['stop_lat']), axis=1)\n",
    "stop_30_mins_final = gpd.GeoDataFrame(stop_30_mins_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stop_30_mins_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loaded_feeds.stops.drop_duplicates(subset='stop_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot bus stop and freqent bus stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the study region boundary gdf\n",
    "shape = gpd.GeoDataFrame.from_file('data/Transport/Australian stops for reference/gtfs_2020_02_14_headway_au_global_indicators.gpkg', \n",
    "                  layer=shape_layer_name)\n",
    "\n",
    "polygon = shape['geometry'].iloc[0]\n",
    "crs = shape.crs['init']\n",
    "\n",
    "polygon_proj = ox.project_geometry(polygon, crs=crs, to_crs=None, to_latlong=True)\n",
    "shape_proj = gpd.GeoDataFrame(polygon_proj[0]).rename(columns={0:'geometry'})\n",
    "shape_proj = gpd.GeoDataFrame(shape_proj,  geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set spatial geometry for all stops \n",
    "loaded_feeds.stops['geometry'] = loaded_feeds.stops.apply(\n",
    "    lambda row: Point(row['stop_lon'], row['stop_lat']), axis=1)\n",
    "all_stops_gdf = gpd.GeoDataFrame(loaded_feeds.stops)\n",
    "\n",
    "# plot the stops\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax = all_stops_gdf.plot(ax=ax, color='green', marker='+', markersize=5, alpha=0.6)\n",
    "ax = stop_30_mins_final.plot(ax=ax, color='red', marker='o', markersize=5, alpha=0.5)\n",
    "ax = shape_proj.plot(ax=ax, color='none', edgecolor='blue')\n",
    "\n",
    "#ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select stops within study region boundary\n",
    "len(stop_30_mins_final[(stop_30_mins_final.intersects(polygon_proj[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all stops within study region boundary\n",
    "len(all_stops_gdf[(all_stops_gdf.intersects(polygon_proj[0]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all cities GTFS frequent stop analysis\n",
    "TODO: move to a python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "GTFS text file header whitespace check completed. Took 3.12 seconds\n",
      "--------------------------------\n",
      "Processing GTFS feed: gtfs_au_sa_adelaidemetro_20191004\n",
      "Successfully converted ['departure_time'] to seconds past midnight and appended new columns to stop_times. Took 3.63 seconds\n",
      "1 GTFS feed file(s) successfully read as dataframes:\n",
      "     gtfs_au_sa_adelaidemetro_20191004\n",
      "     Took 9.15 seconds\n",
      "Start to process adelaide tram analysis during peak_hour\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Complete adelaide (Adelaide Metro) tram during peak_hour with 62 stop counts in 0.6494157314300537 seconds\n",
      "Start to process adelaide bus analysis during peak_hour\n",
      "     Complete adelaide (Adelaide Metro) bus during peak_hour with 5491 stop counts in 8.73745584487915 seconds\n",
      "Start to process adelaide tram analysis during day_hour\n",
      "     Complete adelaide (Adelaide Metro) tram during day_hour with 62 stop counts in 0.6443455219268799 seconds\n",
      "Start to process adelaide bus analysis during day_hour\n",
      "     Complete adelaide (Adelaide Metro) bus during day_hour with 5202 stop counts in 155.36975598335266 seconds\n",
      "--------------------------------\n",
      "GTFS text file header whitespace check completed. Took 28.59 seconds\n",
      "--------------------------------\n",
      "Processing GTFS feed: gtfs_au_vic_ptv_20191004\n",
      "Successfully converted ['departure_time'] to seconds past midnight and appended new columns to stop_times. Took 19.74 seconds\n",
      "1 GTFS feed file(s) successfully read as dataframes:\n",
      "     gtfs_au_vic_ptv_20191004\n",
      "     Took 68.18 seconds\n",
      "Start to process melbourne tram analysis during peak_hour\n",
      "     Complete melbourne (Public Transport Victoria) tram during peak_hour with 1670 stop counts in 4.784363269805908 seconds\n",
      "Start to process melbourne bus analysis during peak_hour\n",
      "     Complete melbourne (Public Transport Victoria) bus during peak_hour with 12196 stop counts in 26.901447534561157 seconds\n",
      "Start to process melbourne tram analysis during day_hour\n",
      "     Complete melbourne (Public Transport Victoria) tram during day_hour with 1670 stop counts in 25.700640201568604 seconds\n",
      "Start to process melbourne bus analysis during day_hour\n"
     ]
    }
   ],
   "source": [
    "# get the work directory\n",
    "dirname = os.path.abspath('') \n",
    "\n",
    "# geopackage path where to save processing layers\n",
    "gpkgPath_output = os.path.join(dirname, 'data/Transport', 'frequent_transit_headway_2020April_python.gpkg')\n",
    "\n",
    "\n",
    "for city in GTFS.keys():\n",
    "    #print(city)\n",
    "    city_config = GTFS['{}'.format(city)]\n",
    "    gtfsfeed_path = city_config['gtfs_filename']\n",
    "    start_date = city_config['start_date_mmdd']\n",
    "    end_date = city_config['end_date_mmdd']\n",
    "    authority = city_config['gtfs_provider']\n",
    "    \n",
    "    loaded_feeds = ua_load.gtfsfeed_to_df(gtfsfeed_path=gtfsfeed_path)\n",
    "    \n",
    "    for hour in ['peak_hour', 'day_hour']:\n",
    "        #print(hour)\n",
    "        stop_frequent = pd.DataFrame()\n",
    "        \n",
    "        \n",
    "        for mode in ['tram', 'bus']:\n",
    "            startTime = time.time()\n",
    "            print('Start to process {} {} analysis during {}'.format(city, mode, hour))\n",
    "            \n",
    "            hour_1 = city_config['modes']['{}'.format(mode)]['{}'.format(hour)]\n",
    "            start_hour = hour_1[0]\n",
    "            end_hour = hour_1[1]\n",
    "\n",
    "            headway_intervals = city_config['modes']['{}'.format(mode)]['intervals']\n",
    "            route_types = city_config['modes']['{}'.format(mode)]['route_types']\n",
    "            agency_ids = city_config['modes']['{}'.format(mode)]['agency_id']\n",
    "            \n",
    "            stops_headway = get_hlc_stop_frequency(loaded_feeds, start_hour, end_hour, start_date, \n",
    "                               end_date, route_types, agency_ids, \n",
    "                               dow=['monday','tuesday','wednesday','thursday','friday'])\n",
    "\n",
    "            # select average departure headway less than or equal to maxi headway frequency intervals\n",
    "            stop_frequent_headway = stops_headway[stops_headway['headway'] <= headway_intervals]\n",
    "            \n",
    "            if len(stop_frequent_headway) > 0:\n",
    "                stop_frequent_final = pd.merge(stop_frequent_headway, loaded_feeds.stops, how='left', on='stop_id')\n",
    "                stop_frequent_final['authority'] = authority\n",
    "                stop_frequent_final['mode'] = mode\n",
    "                stop_frequent = stop_frequent.append(stop_frequent_final)\n",
    "                print('     Complete {} ({}) {} during {} with {} stop counts in {} seconds'.format(\n",
    "                    city, authority, mode, hour, len(stop_frequent_headway), time.time() - startTime))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # get spatial features for freqent stops\n",
    "        # add stop id geometry\n",
    "        stop_frequent['geometry'] = stop_frequent.apply(\n",
    "            lambda row: Point(row['stop_lon'], row['stop_lat']), axis=1)\n",
    "        stop_frequent_gdf = gpd.GeoDataFrame(stop_frequent)\n",
    "\n",
    "\n",
    "        # save to output file\n",
    "        # save the frequent stop by study region and modes to a new layer in geopackage\n",
    "        stop_frequent_gdf.to_file(\n",
    "            gpkgPath_output,\n",
    "            layer='{}_{}min_stops_{}_{}_{}'.format(\n",
    "                city, headway_intervals, hour, start_date, end_date),\n",
    "            driver='GPKG')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GlobalInd)",
   "language": "python",
   "name": "globalind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
